# supercomp-group18
### Software Improvements
Considering the fact that the DataFrame implementation performed better in the experiments that we ran locally, we opted against an RDD implementation. Moreover, given that Spark SQL is especially designed and optimized to run with structured data we modified the previous DataFrame implementation into one that took advantage more of the benefits of Datasets in Spark SQL. Thereby, in this section we will present the changes that we made in order to achieve a better performance.

The former approach had all the logic encompassed into a single map function which received as input a row containing a date and the aggregated list of strings corresponding to the "allnames" entries of that particular date and was giving as an output a tuple which contained the date and the required list of tuples (string, number of apparitions). This was barely using the Spark SQL capabilities so it was changed drastically.

Currently, in order to only perform operation on the data that we are interested in, a select was performed to isolate the "date" and "allnames" columns. By doing so, the following explode operation, will only copy the date field for the resulting split strings, instead of copying also the remaining 25 unused fields. Moreover, the split is not based anymore on comma and semicolon, but it contains a regular expression that matches every string starting with a comma and ending with non-letter characters. This was done so that the number resulting strings after split will be significantly lowered since the numbers are now completely ignored. However, a filter for strings that contained letters was still performed to ensure that no other characters were kept. Following, the occurrences of words by date was computed by grouping the dataset by date and names and adding the result to a third column. The extraction of the top 10 counted words by date was performed by using a query with window aggregation function. The rank of an entry was defined in relation to its word count and the DateSet was partitioned by date. The qurey returned the top 10 ranked entries from each partition. On the resulting dataset, the counts and the names were merged and, finally, the data was grouped by date and aggregated into a list. To manage the number of output files that the script was writing, the dataset was repartitioned into one partition so that the result will be just one file. 

### Configuration

In a company scenario we considered that an optimum configuration would be a trade-off between the execution time and required costs.

Our approach was to first process the entire data-set using the configuration mentioned in the assignment's manual, namely using 20 **c4.8xlarge** core nodes. The processing time obtained was 430 seconds which, by conversion, means 7 minutes and 10 seconds.
